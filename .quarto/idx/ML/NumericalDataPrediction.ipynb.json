{"title":"Multiple Linear Regression: Application on FARS Dataset","markdown":{"yaml":{"title":"Multiple Linear Regression: Application on FARS Dataset"},"headingText":"Mathematical Formulation","containsRefs":false,"markdown":"\n\n\n\n\n\nWhen modeling the relationship between a dependent variable $y$ and multiple independent variables $x_1, x_2, \\dots, x_n$, the linear equation can be written as:\n\\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\n\nwhere:\n\n- $y_i$ is the observed value of the dependent variable for the $i^{th}$ observation.\n\n- $x_1, x_2, \\dots, x_n$ are the values of the independent variables for the $i^{th}$ observation.\n\n- $\\beta_0, \\beta_1, \\beta_n$ are the regression coefficients, with $\\beta_0$ being the y-intercept.\n\n- $\\epsilon_i$ is the error term for the $i^{th}$ observation, capturing the difference between the observed value and the value predicted by the model.\n\n\nThe requirement here is to find the $\\beta_i$ that minimizes the meas square error of $\\epsilon_i$. \nThe above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU's can deal faster with matrix manipulation.\n\n### Matrix Notation\n\nGiven:\n\n- $\\boldsymbol{X}$ is the design matrix of size $m \\times (n + 1)$.\n\n- $\\boldsymbol{y}$ is a column vector of size $m \\times 1$ containing the dependent variable values.\n\n- $\\boldsymbol{\\beta}$ is a column vector of size $(n + 1) \\times 1$ containing the regression coefficients.\n\n- $\\boldsymbol{\\epsilon}$  is a column vector of size $m \\times 1$ representing the errors.\n\n\n\nThe relationship is given by:\n\\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\n\nTo determine the value of $\\beta$, one might think to rearrange the equation as:\n\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\n\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here's a breakdown of the issues:\n\n\n- **Matrix Division:** In the realm of matrices, there isn't a direct concept of division like there is with regular numbers. So, saying $\\beta = \\frac{y}{X}$ doesn't have a straightforward meaning. \n\n- **Matrix Multiplication:** Matrix multiplication is not commutative. This means that the product $AB$ is not necessarily the same as the product $BA$. So, even if we were to try to \"isolate\" $\\beta$ by some matrix operation, it wouldn't be as simple as dividing both sides by $X$. \n\n- **Correct Approach:** The correct way to \"isolate\" $\\beta$ when dealing with matrices is to multiply both sides of the equation by the inverse of $X$ (assuming $X$ is invertible). The equation would look something like: $\\beta = X^{-1}y$. \n    Note that this equation assumes that $X$ is a square matrix and has an inverse. If $X$ is not square, or doesn't have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate $\\beta$. \n    \n- **Dimensionality:** Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation $y = X\\beta + \\epsilon$, $y$ is a column vector of size $m \\times 1$, $X$ is a matrix of size $m \\times n$, and $\\beta$ is a column vector of size $n \\times 1$. Dividing an $m \\times 1$ vector by an $m \\times n$ matrix doesn't produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ \\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $ instead of \n$ \\boldsymbol{\\beta} = \\mathbf{X}^{-1} \\mathbf{y} $ is due to the structure and properties of the design matrix \\( \\mathbf{X} \\) in linear regression.\n\n\n- **Non-Square Matrix:** In most real-world applications of linear regression, $\\mathbf{X}$ is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, $\\mathbf{X}^{-1}$ doesn't exist for these cases.\n\n- **Pseudo-Inverse:** The expression $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ is known as the Moore-Penrose pseudo-inverse of $\\mathbf{X}$. This pseudo-inverse provides a means to approximate an inverse. \n\n- **Projection onto Column Space:** The term $\\mathbf{X}^T \\mathbf{y}$ can be interpreted as projecting the response vector $\\mathbf{y}$ onto the column space of $\\mathbf{X}$. \n\n- **Minimization of Residuals:** The expression $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ originates from differentiating the sum of squared residuals with respect to $\\boldsymbol{\\beta}$ and setting it to zero.\n\n\nWe'll be applying this concept to the FARS dataset. Managed by the U.S. National Highway Traffic Safety Administration (NHTSA), the Fatality Analysis Reporting System (FARS) provides an extensive record of fatal motor vehicle crashes in the U.S. and Puerto Rico since 1975.\n\n\n\n## Code\n\n### Describe the FARS (Fatality Analysis Reporting System) dataset\n\nOur analysis will encompass six specific tables within this dataset. The tables and their respective descriptions are as follows:\n\n- Person: This table provides detailed information on the individuals involved in the accident.\n- Drugs: This table presents data on post-mortem toxicology results, indicating the presence of drugs in the bloodstream of the deceased.\n- Race: This table delineates the ethnic background of the individuals involved in the accident.\n- Vision: This table offers insights into the visual clarity at the accident site.\n- Weather: This table details the meteorological conditions at the time of the accident.\n- Maneuver: This table ascertains whether the drivers were executing a maneuver at the time of the incident and provides specifics about the maneuver in question.\n\nThe variables description is provided [here](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813417)\n\n### Describtive Statistics\n\nIn analyzing the monthly distribution of road accidents, the data reveals distinct patterns throughout the year. March registers the lowest count, slightly above 6,000, while June and August exhibit peaks, nearing or surpassing 10,000 accidents. The trend shows a moderate start in January, a dip in March, escalating to a climax in June, a minor decrease in July, another peak in August, and a decline up to November with a minor rise in December. Potential influencing factors include varying weather conditions, increased road traffic during summer vacation periods in June and August, and heightened travel during December's festive season.\n","srcMarkdownNoYaml":"\n\n\n\n\n## Mathematical Formulation\n\nWhen modeling the relationship between a dependent variable $y$ and multiple independent variables $x_1, x_2, \\dots, x_n$, the linear equation can be written as:\n\\begin{equation}\ny_i=\\beta_0+\\beta_1 x_{i 1}+\\beta_2 x_{i 2}+\\cdots+\\beta_n x_{i n}+\\epsilon_i\n\\end{equation}\n\nwhere:\n\n- $y_i$ is the observed value of the dependent variable for the $i^{th}$ observation.\n\n- $x_1, x_2, \\dots, x_n$ are the values of the independent variables for the $i^{th}$ observation.\n\n- $\\beta_0, \\beta_1, \\beta_n$ are the regression coefficients, with $\\beta_0$ being the y-intercept.\n\n- $\\epsilon_i$ is the error term for the $i^{th}$ observation, capturing the difference between the observed value and the value predicted by the model.\n\n\nThe requirement here is to find the $\\beta_i$ that minimizes the meas square error of $\\epsilon_i$. \nThe above could be solved in multiple ways. However, one easy way to solve it is through matrix multiplication since CPU's can deal faster with matrix manipulation.\n\n### Matrix Notation\n\nGiven:\n\n- $\\boldsymbol{X}$ is the design matrix of size $m \\times (n + 1)$.\n\n- $\\boldsymbol{y}$ is a column vector of size $m \\times 1$ containing the dependent variable values.\n\n- $\\boldsymbol{\\beta}$ is a column vector of size $(n + 1) \\times 1$ containing the regression coefficients.\n\n- $\\boldsymbol{\\epsilon}$  is a column vector of size $m \\times 1$ representing the errors.\n\n\n\nThe relationship is given by:\n\\begin{equation}\n\\large{y} = \\large{X} \\beta + \\epsilon\n\\end{equation}\n\nTo determine the value of $\\beta$, one might think to rearrange the equation as:\n\n\\begin{equation}\n\\beta = \\frac{\\large{y}}{\\large{X}}\n\\end{equation}\n\nHowever, this representation is not accurate in the context of matrix operations. The inaccuracy arises due to the nature of matrices and how they are manipulated. Here's a breakdown of the issues:\n\n\n- **Matrix Division:** In the realm of matrices, there isn't a direct concept of division like there is with regular numbers. So, saying $\\beta = \\frac{y}{X}$ doesn't have a straightforward meaning. \n\n- **Matrix Multiplication:** Matrix multiplication is not commutative. This means that the product $AB$ is not necessarily the same as the product $BA$. So, even if we were to try to \"isolate\" $\\beta$ by some matrix operation, it wouldn't be as simple as dividing both sides by $X$. \n\n- **Correct Approach:** The correct way to \"isolate\" $\\beta$ when dealing with matrices is to multiply both sides of the equation by the inverse of $X$ (assuming $X$ is invertible). The equation would look something like: $\\beta = X^{-1}y$. \n    Note that this equation assumes that $X$ is a square matrix and has an inverse. If $X$ is not square, or doesn't have an inverse, other methods like the Moore-Penrose pseudoinverse would be used to estimate $\\beta$. \n    \n- **Dimensionality:** Even if we were to entertain the idea of matrix division, the dimensions must be compatible. In the equation $y = X\\beta + \\epsilon$, $y$ is a column vector of size $m \\times 1$, $X$ is a matrix of size $m \\times n$, and $\\beta$ is a column vector of size $n \\times 1$. Dividing an $m \\times 1$ vector by an $m \\times n$ matrix doesn't produce a consistent result in terms of matrix dimensions.\n\nThe reason we use the equation $ \\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $ instead of \n$ \\boldsymbol{\\beta} = \\mathbf{X}^{-1} \\mathbf{y} $ is due to the structure and properties of the design matrix \\( \\mathbf{X} \\) in linear regression.\n\n\n- **Non-Square Matrix:** In most real-world applications of linear regression, $\\mathbf{X}$ is not a square matrix. It usually has more rows (observations) than columns (predictors). Only square matrices possess inverses in the traditional sense. Therefore, $\\mathbf{X}^{-1}$ doesn't exist for these cases.\n\n- **Pseudo-Inverse:** The expression $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ is known as the Moore-Penrose pseudo-inverse of $\\mathbf{X}$. This pseudo-inverse provides a means to approximate an inverse. \n\n- **Projection onto Column Space:** The term $\\mathbf{X}^T \\mathbf{y}$ can be interpreted as projecting the response vector $\\mathbf{y}$ onto the column space of $\\mathbf{X}$. \n\n- **Minimization of Residuals:** The expression $(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T$ originates from differentiating the sum of squared residuals with respect to $\\boldsymbol{\\beta}$ and setting it to zero.\n\n\nWe'll be applying this concept to the FARS dataset. Managed by the U.S. National Highway Traffic Safety Administration (NHTSA), the Fatality Analysis Reporting System (FARS) provides an extensive record of fatal motor vehicle crashes in the U.S. and Puerto Rico since 1975.\n\n\n\n## Code\n\n### Describe the FARS (Fatality Analysis Reporting System) dataset\n\nOur analysis will encompass six specific tables within this dataset. The tables and their respective descriptions are as follows:\n\n- Person: This table provides detailed information on the individuals involved in the accident.\n- Drugs: This table presents data on post-mortem toxicology results, indicating the presence of drugs in the bloodstream of the deceased.\n- Race: This table delineates the ethnic background of the individuals involved in the accident.\n- Vision: This table offers insights into the visual clarity at the accident site.\n- Weather: This table details the meteorological conditions at the time of the accident.\n- Maneuver: This table ascertains whether the drivers were executing a maneuver at the time of the incident and provides specifics about the maneuver in question.\n\nThe variables description is provided [here](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813417)\n\n### Describtive Statistics\n\nIn analyzing the monthly distribution of road accidents, the data reveals distinct patterns throughout the year. March registers the lowest count, slightly above 6,000, while June and August exhibit peaks, nearing or surpassing 10,000 accidents. The trend shows a moderate start in January, a dip in March, escalating to a climax in June, a minor decrease in July, another peak in August, and a decline up to November with a minor rise in December. Potential influencing factors include varying weather conditions, increased road traffic during summer vacation periods in June and August, and heightened travel during December's festive season.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"toc-depth":2,"html-math-method":"katex","output-file":"NumericalDataPrediction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"darkly","toc-expand":2,"max-width":"500px","code-summary":"Show the code","title":"Multiple Linear Regression: Application on FARS Dataset"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}